---
title: "Supervised Learning II - Demonstration"
author: "PLSC 21510/31510"
date: "Fall 2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Setting up New York Times Annotated Corpus

### 1.1

Today, we are going to analyze the New York Times Annotated Corpus. The data is stored in an `.RData` file.

```{r}
load("data/NYT.RData")
```

This loads a list, `nyt_list`, with the following components:
- `train` : the document term matrix for the training set
- `train_label`: an indicator equal to 1 if the story comes from the national desk for each document in the training set
- `test`: the document term matrix for the test set.  
- `test_label`: an indicator equal to 1 if the story comes from the national desk for each document in the test set

We will work with `train` and `train_label` to build our prediction models. We will use the `test` set to test the fit of our model.  

Let's put these components in individual objects.

```{r}
train <- nyt_list[[1]]
train_label <- nyt_list[[2]]
test <- nyt_list[[3]]
test_label <- nyt_list[[4]]
```

### 1.2 

How many observations do we have in the training and test sets?

```{r}
# YOUR CODE HERE
```

### 1.3

Note that the `train` and `test` matrices do not contain a column for the labels. The code below combines the dtm and labels into a data frame for the train and test sets.
```{r}
train.df <- as.data.frame(cbind(train, train_label))
test.df <- as.data.frame(cbind(test, test_label))
```

## 2. Linear Probability Model

### 2.1 

We are ready to apply a linear probability model to perform classification. Using the `lm` function regress `train_label` against all the words in train. To do this, note that you can include all the variables in a regression using the following syntax:
`full_reg<- lm(train_label ~ . , data = train.df)`

The `~.` tells R to include all the variables in the data frame.

```{r}
full_reg <- lm(train_label ~ . , data = train.df)
```

### 2.2

Summarize the model. What do you notice?

```{r}
# YOUR CODE HERE TO SUMMARIZE

# Count the number of coefficients dropped from the model
length(which(is.na(full_reg$coeff)==T))
```

### 2.3

We are now going to make predictions using the training data and the test data and compare their properties.

Using the `predict` function, make predictions for all observations in the training set.  

```{r}
train_pred <- predict(full_reg, as.data.frame(train))
```

### 2.4

Then, classify the documents as national or not using a threshold of 0.5.

```{r}
class_doc <- ifelse(train_pred > 0.5, 1, 0)
```

### 2.5

Assess your classification to the actual data with `table`. What do you notice?

```{r}
# YOUR CODE HERE
```

### 2.6

Now, we  use the model to make predictions for the *test* data and classify using a 0.5 threshold.

Run the code below to assess the accuracy of your classification by comparing it to the actual test data. What do you notice?

```{r}
# predict test observations
test_pred<- predict(full_reg, as.data.frame(test))

# classify using threshold of 0.5
class_pred<- ifelse(test_pred > 0.5, 1, 0)

# create confusion matrix
table(class_pred, test_label)

# Get accuracy score of our predictions
(sum(class_pred & test_label) + sum(!class_pred & !test_label)) / length(test_label)

# That is really low accuracy! In fact we could just guess and do better:
set.seed(123)
rand_guess <- rbinom(length(test_label), prob = 0.25, size = 1)
sum(diag(table(rand_guess, test_label)))/length(test_label)
```

STOP!

## 3. Fit LASSO regression

### 3.1

We are going to use the `glmnet` library to fit the LASSO regression. Run the code below to install and load the required library.

```{r}
# uncomment to install
# install.packages('glmnet')
library(glmnet)
```

### 3.2 

The syntax for the glmnet model is as follows:
`lasso <- glmnet(x = train, y = train_label)`

This defaults to linear regression. To do logistic regression you can fit the same model, but add
`lasso_logist <- glmnet(x = train, y = train_label, family = 'binomial')`

Fit a LASSO linear regression.

```{r}
lasso <- glmnet(x = train, y = train_label)
```

### 3.3 

The LASSO function automatically fits the model for several values of lambda.  

Using the `colSums` function, sum up the columns of `lasso$beta`. Plot that against `lasso$lambda`. What generally happens as lambda increases?  

```{r}
# sum absolute values of the betas
sum_beta <- colSums(abs(lasso$beta))

# plot against values of lambda
plot(sum_beta ~ lasso$lambda)
```

### 3.4 

Later, we'll discuss methods for selecting lambda.  For now, we're going to set a particular value of lambda arbitrarily and then assess its performance. We will set lambda to 0.05.

Formulate predictions for the training set using the following syntax:
`lasso_pred <- predict(lasso, newx = train, s = 0.05 )`

- `lasso` is the lasso regression
- `newx` are the values you want to predict
- `s` is the value of lambda.

Classify the observations using a threshold of 0.5. Then assess the accuracy of those predictions by comparing them to the training set labels.

```{r}
# formulate predictions
lasso_pred <- predict(lasso, newx=train, s = 0.05 )

# classify obs using threshold of 0.5
class_lasso <- ifelse(lasso_pred>0.5, 1, 0)

# create confusion matrix
table(class_lasso, train_label)

# get accuracy score
(sum(class_lasso & train_label) + sum(!class_lasso & !train_label)) / length(train_label)

## Notice that we don't have the perfect in-sample fit that we had before.
## This is a good thing!  It means that we're not overfitting our data. 
```

### 3.5 

Now formulate predictions for the test set,  classify the documents as national or not with a threshold of 0.5, and assess the accuracy of those predictions by comparing them to the test set labels.  What do you notice about the quality of the predictions from LASSO relative to the predictions from OLS?

```{r}
# formulate predictions
lasso_test <- predict(lasso, newx=test, s = 0.05 )

# classify obs using threshold of 0.5
class_lasso_test <- ifelse(lasso_test>0.5, 1, 0)

# create confusion matrix
table(class_lasso_test, test_label)

# get accuracy score
(sum(class_lasso_test & test_label) + sum(!class_lasso_test & !test_label)) / length(test_label)

## That is much better out of sample accuracy! (but notice lower than our in sample accuracy).
```


## 4. Finding Lambda

### 4.1

We're now ready to devise a method for selecting the appropriate value of lambda. First we need to define our loss function. To do this, write a function calculate the mean squared error:

```{r}
mse <- function(preds, data){
  diff <- preds - data
  diff_squared<- diff^2
  return(mean(diff_squared))
}
```

### 4.2

Now, let's calculate the MSE for the in-sample fit from the LASSO regression. 

1. Make in-sample predictions using LASSO for each value of lambda. 
2. Then calculate the MSE across those predictions. 
3. Finally, make a plot of the MSE values against lambda values.

```{r}
# make predictions for each value of lamda
pred_lasso <- predict(lasso, newx = train) 

# calculate MSE for each lamda value
store_mse_in <- c()
for(z in 1:ncol(pred_lasso)){
  predictions <- pred_lasso[,z] #get predictors for that lambda value
  store_mse_in[z] <- mse(predictions, train_label)
}

# plot MSE x lamda
plot(store_mse_in~lasso$lambda)
```

Recalling that smaller MSE is better, what does the insample fit tell us is the optimal lambda value? How much are we penalizing the model then?

## 5. Cross Validation

### 5.1 

We want to devise a way to do cross validation. With LASSO we will have a canned method for doing the cross validation. 

(I provide instructions below on how to manually perform cross validation---helpful for applying the procedure to many other methods.)

To perform cross validation with `glmnet` we use `cv.glmnet`

```{r}
lasso_cv <- cv.glmnet(x = train, y = train_label)
```

You can specify the loss function (with `type.measure =` ). For example, for classification you might pick accuracy.  The default is MSE. `nfolds` allows you to set the number of folds used for cross validation.

There are lots of built in functions in `cv.glmnet`. Try plotting the `cv.glmnet` object you created. This shows how the Mean-Squared Error for the cross validated predictions changes across different values of lambda.

```{r}
plot(lasso_cv)
```

We can access the lambdas that lead to the smallest mse with `obj$lambda.min` and the lambda values attempted with `obj$lambda`

```{r}
lasso_cv$lambda
lasso_cv$lambda.min
```

### 5.2 

The plot object gave us the in sample fit.  Let's see how it compares to the out-of-sample fit.

To do this, make predictions for the test data using each value of lambda from cv.glmnet

```{r}
out_of_sample<- c()
for(z in 1:length(lasso_cv$lambda)){
  preds1<- predict(lasso_cv, newx = test, s= lasso_cv$lambda[z])
  out_of_sample[z]<- mse(preds1, test_label)
}
```

### 5.3

Plot the out-of-sample mse against the estimated mse from cross validation (which you can access with `obj$cvm`).

```{r}
plot(lasso_cv$cvm ~ lasso_cv$lambda) 
plot(out_of_sample ~ lasso_cv$lambda)
```

How well did cross validation do in selecting the appropriate value of lambda?


## Bonus section: your own cross validation 

You can fit your own cross validations using the following procedure:

1) Use the sample function with `replace = T` to assign each observation to a fold
2) Use a for loop to train on the K-1 subsets of the data
3) Predict for the held out data for that round
4) I then store those out of sample predictions

Using this procedure, manually determine how many of the top occurring words you should include to make the best mean squared error predictions

```{r}
#first, determining the folds
set.seed(2636815)
folds <- sample(1:10, nrow(train), replace=T) # notice I'm sampling with replacement
```

Now, let's write a for loop for fitting the model:

```{r}
# note, that the top 10 words might change depending on the cross validation run
store_pred<- rep(NA,nrow(train))
for(z in 1:10){
  cv_train_set<- which(folds!=z) # these are the "training data"
  cv_test_set<- which(folds==z) # these are the "test data"
  counts<- apply(train[cv_train_set, ], 2, sum)
  top_10 <- order(counts, decreasing=T)[1:10]
  reg1<- lm(train_label[cv_train_set]~., data = as.data.frame(train[cv_train_set, top_10]))
  store_pred[cv_test_set]<- predict(reg1, newdata = as.data.frame(train[cv_test_set,]))
  
}

out_mse <- mse(train_label, store_pred)
```
