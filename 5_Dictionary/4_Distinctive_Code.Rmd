---
title: "Distinctive Words"
author: "PLSC 21510/31510"
date: "Fall 2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Prepare packages

### 1.1 

Download and install the following packages to get started:

```{r}
require(quanteda)
require(quanteda.corpora)
require(quanteda.textstats)
require(quanteda.textplots)
require(matrixStats) # for statistics
```

### 1. 2

Today we're going to find distinctive words in the speeches of Obama and Trump.

Run the following code to:
1. Import the corpus
2. Create a DTM

```{r}
# import
trump_obama <- corpus_subset(data_corpus_sotu, President %in% c("Trump", "Obama")) 
summary(trump_obama)

# process
# preprocess and create DTM
toks <- trump_obama %>%
  tokens(split_hyphens = T,
         remove_punct = TRUE,
         remove_numbers = TRUE,
         remove_symbols = TRUE) %>%
  tokens_tolower() %>% 
  tokens_remove(pattern = stopwords("en")) %>%
  tokens_wordstem()
dfm_ot <- dfm(toks)

# dimensions
nfeat(dfm_ot)
ndoc(dfm_ot)
```

## 2. Measuring "distinctiveness"

Oftentimes scholars will want to compare different corpora by finding the words (or features) distinctive to each corpora. But finding distinctive words requires a decision about what “distinctive” means. As we will see, there are a variety of definitions that we might use. 

### 2.1 Unique usage

The most obvious definition of distinctive is "exclusive". That is, distinctive words are those are found exclusively in texts associated with a single author (or group). For example, if Trump uses the word “access” and Obama never does, we should count “access” as distinctive. 

Finding words that are exclusive to a group is a simple exercise. All we have to do is sum the usage of each word use across all texts for each author, and then look for cases where the sum is zero for one author.

```{r}
# see presidents
docvars(dfm_ot, "President")

# convert to data frame
dfm_m <- convert(dfm_ot, to = "matrix")

# Subset into 2 dtms for each author
obama <- dfm_m[1:8,]
trump <- dfm_m[9:12,]

# Sum word usage counts across all texts
obama <- colSums(obama)
trump <- colSums(trump)

# Put those sums back into a dataframe
df <- data.frame(rbind(obama,trump))
df[,1:5]

# Get words where one author's usage is 0
solelyobama <- unlist(df[1,trump==0]) 
solelyobama <- solelyobama[order(solelyobama, decreasing = T)] # order them by frequency
head(solelyobama, 10) # get top 10 words for obama

solelytrump <- unlist(df[2,obama==0])
solelytrump <- solelytrump[order(solelytrump, decreasing = T)] # order them by frequency
head(solelytrump, 10) # get top 10 words for trump
```

### 2.2 Removing unique words

As we can see, these words tend not to be terribly interesting or informative. So we will remove them from our corpus in order to focus on identifying distinctive words that appear in texts associated with every author.

```{r}
# subset df with non-zero entries
df <- df[,trump>0 & obama>0]

# how many words are we left with?
ncol(df)
df[,1:5]
```

### 2.3 Differences in frequences

Another basic approach to identifying distinctive words is to compare the frequencies at which authors use a word. If one author uses a word often across his or her oeuvre and another barely uses the word at all, the difference in their respective frequencies will be large. We can calculate this quantity the following way:

```{r}
# take the differences in frequences
diffFreq <- obama - trump

# sort the words
diffFreq <- sort(diffFreq, decreasing = T)

# the top obama words
head(diffFreq, 10)

# the top trump words
tail(diffFreq, 10)
```

### 2.4 Differences in averages

This is a good start. But what if one author uses more words *overall*? Instead of using raw frenquncies, a better approach would look at the average *rate* at which authors use various words. 

We can calculate this quantity the following way:

1. Normalize the DTM from counts to proportions
2. Take the difference between one author's proportion of a word and another's proportion of the same word.
3. Find the words with the highest absolute difference.

```{r}
# normalize into proportions
rowTotals <- rowSums(df) #create vector with row totals, i.e. total number of words per document
head(rowTotals) # notice that one author uses more words than the other

# change frequencies to proportions
df <- df/rowTotals #change frequencies to proportions
df[,1:5]

# get difference in proportions
means.obama <- df[1,]
means.trump <- df[2,]
score <- unlist(means.obama - means.trump)

# find words with highest difference
score <- sort(score, decreasing = T)
head(score,10) # top obama words
tail(score,10) # top trump words
```

This is a start. The problem with this measure is that it tends to highlight differences in very frequent words. For example, this method gives greater attention to a word that occurs 30 times per 1,000 words in Obama and 25 times per 1,000 in Trump than it does to a word that occurs 5 times per 1,000 words in Obama and 0.1 times per 1,000 words in Trump. This does not seem right. It seems important to recognize cases when one author uses a word frequently and another author barely uses it.

As this initial attempt suggests, identifying distinctive words will be a balancing act. When comparing two groups of texts, differences in the rates of frequent words will tend to be large relative to differences in the rates of rarer words. Human language is variable; some words occur more frequently than others regardless of who is writing. We need to find a way of adjusting our definition of distinctive in light of this.

### 2.5 Difference in averages, adjustment

One adjustment that is easy to make is to divide the difference in authors’ average rates by the average rate across all authors. Since dividing a quantity by a large number will make that quantity smaller, our new distinctiveness score will tend to be lower for words that occur frequently. While this is merely a heuristic, it does move us in the right direction.

```{r}
# get the average rate of all words across all authors
means.all <- colMeans(df)

# now divide the difference in authors' rates by the average rate across all authors
score <- unlist((means.obama - means.trump) / means.all)
score <- sort(score, decreasing = T)
head(score,10) # top obama words
tail(score,10) # top trump words
```

## 3. "Keyness" 

### Challenge 1

`quanteda` has a function called `textstat_keyness`. Read the documentation and use it to find distinctive terms for Obama and Trump. Try out different measures. Use `textplot_keyness` to plot those terms.

```{r}
?textstat_keyness

# YOUR CODE HERE
```




